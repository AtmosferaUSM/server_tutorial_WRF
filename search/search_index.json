{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This tutorial will cover from the cluster creation using PClusterManager to the configuration and compilation of WRF Version 4.3.3 (Weather Research and Forecasting) and WPS version 4.3.1 (WRF Preprocessing System) using Spack. Create Cluster We have to create a computer cluster on AWS to install and run WRF. The links to AWS and PCluster Managers are below. Go to the PCluster link to create the cluster after having a account on AWS. Link to AWS Link to PCluster Manager Upload YML file PCluster Manager makes it easy for us to create and manage the clusters through interface. This is an example configuration. For your first time, you may choose to upload the yml file we have prepared for you and configure to your prefered headnode and shared storage later by updating the yaml file further. In order for AWS to verify your identity, you will need to create your own EC2 Key Pair using Amazon EC2. YAML File 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 HeadNode : InstanceType : c5a.xlarge Ssh : KeyName : PClusterManager Networking : SubnetId : subnet-0412e7315562aa1da LocalStorage : RootVolume : VolumeType : gp3 Size : 50 Iam : AdditionalIamPolicies : - Policy : arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore - Policy : arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess Dcv : Enabled : true Scheduling : Scheduler : slurm SlurmQueues : - Name : queue0 ComputeResources : - Name : queue0-hpc6a48xlarge MinCount : 0 MaxCount : 10 InstanceType : hpc6a.48xlarge Efa : Enabled : true GdrSupport : true DisableSimultaneousMultithreading : true Networking : SubnetIds : - subnet-01a39ae1f7194644a PlacementGroup : Enabled : true ComputeSettings : LocalStorage : RootVolume : VolumeType : gp3 Size : 50 Iam : AdditionalIamPolicies : - Policy : arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess Region : us-east-2 Image : Os : alinux2 SharedStorage : - Name : Ebs0 StorageType : Ebs MountDir : /shared EbsSettings : VolumeType : gp3 DeletionPolicy : Delete Size : '200' Encrypted : false Create Cluster Dry Run You can opt to dry run your cluster before creating it. After dry run, you will see this message printed: Request would have succeeded, but DryRun flag is set . Create Creating cluster will take around 15-20 minutes. You may check if you are interested in the steps or current progress of stack events. Create in Progress Create Complete Now you should be able to see the cluster WRF in CREATE_COMPLETE status from your PCluster Manager interface. Click on Shell and login to your own AWS account to access the terminal.","title":"Create Cluster"},{"location":"#create-cluster","text":"We have to create a computer cluster on AWS to install and run WRF. The links to AWS and PCluster Managers are below. Go to the PCluster link to create the cluster after having a account on AWS. Link to AWS Link to PCluster Manager","title":"Create Cluster"},{"location":"#upload-yml-file","text":"PCluster Manager makes it easy for us to create and manage the clusters through interface. This is an example configuration. For your first time, you may choose to upload the yml file we have prepared for you and configure to your prefered headnode and shared storage later by updating the yaml file further. In order for AWS to verify your identity, you will need to create your own EC2 Key Pair using Amazon EC2. YAML File 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 HeadNode : InstanceType : c5a.xlarge Ssh : KeyName : PClusterManager Networking : SubnetId : subnet-0412e7315562aa1da LocalStorage : RootVolume : VolumeType : gp3 Size : 50 Iam : AdditionalIamPolicies : - Policy : arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore - Policy : arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess Dcv : Enabled : true Scheduling : Scheduler : slurm SlurmQueues : - Name : queue0 ComputeResources : - Name : queue0-hpc6a48xlarge MinCount : 0 MaxCount : 10 InstanceType : hpc6a.48xlarge Efa : Enabled : true GdrSupport : true DisableSimultaneousMultithreading : true Networking : SubnetIds : - subnet-01a39ae1f7194644a PlacementGroup : Enabled : true ComputeSettings : LocalStorage : RootVolume : VolumeType : gp3 Size : 50 Iam : AdditionalIamPolicies : - Policy : arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess Region : us-east-2 Image : Os : alinux2 SharedStorage : - Name : Ebs0 StorageType : Ebs MountDir : /shared EbsSettings : VolumeType : gp3 DeletionPolicy : Delete Size : '200' Encrypted : false","title":"Upload YML file"},{"location":"#create-cluster_1","text":"Dry Run You can opt to dry run your cluster before creating it. After dry run, you will see this message printed: Request would have succeeded, but DryRun flag is set . Create Creating cluster will take around 15-20 minutes. You may check if you are interested in the steps or current progress of stack events.","title":"Create Cluster"},{"location":"#create-in-progress","text":"","title":"Create in Progress"},{"location":"#create-complete","text":"Now you should be able to see the cluster WRF in CREATE_COMPLETE status from your PCluster Manager interface. Click on Shell and login to your own AWS account to access the terminal.","title":"Create Complete"},{"location":"delete_spack/","text":"Delete existing Spack for fresh installation Export variable You can just remove the existing $SPACK_ROOT and install a new Spack environment. For example, if $SPACK_ROOT is /shared/spack , you can do: export SPACK_ROOT=/shared/spack Remove spack from the system rm -rf $SPACK_ROOT Download Spack Now, you can download Spack and go through the steps of Spack Installation again. export SPACK_ROOT=/shared/spack mkdir -p $SPACK_ROOT git clone -b v0.18.0 -c feature.manyFiles=true https://github.com/spack/spack $SPACK_ROOT","title":"Delete Spack Folder"},{"location":"delete_spack/#delete-existing-spack-for-fresh-installation","text":"","title":"Delete existing Spack for fresh installation"},{"location":"delete_spack/#export-variable","text":"You can just remove the existing $SPACK_ROOT and install a new Spack environment. For example, if $SPACK_ROOT is /shared/spack , you can do: export SPACK_ROOT=/shared/spack","title":"Export variable"},{"location":"delete_spack/#remove-spack-from-the-system","text":"rm -rf $SPACK_ROOT","title":"Remove spack from the system"},{"location":"delete_spack/#download-spack","text":"Now, you can download Spack and go through the steps of Spack Installation again. export SPACK_ROOT=/shared/spack mkdir -p $SPACK_ROOT git clone -b v0.18.0 -c feature.manyFiles=true https://github.com/spack/spack $SPACK_ROOT","title":"Download Spack"},{"location":"geogrid/","text":"Geogrid Preliminary Before we start executing geogrid.exe, ungrib.exe and metgrid.exe, create a folder named WRF_Resources to put all the required input data for WPS. We will download all the required data under the /scratch folder. cd /shared/scratch mkdir WRF_Resources cd WRF_Resources Download the geogrid data You can download the mandatory high resolution data from the official website of UCAR: Download data for the input of geogrid.There are two types of data sets available, each made up of the hightest and lowest resolution of each mandatory field. You may read the details on the official website to decide on the data that suits your need. In this case, we will be using the highest resolution of each mandatory field. wget https://www2.mmm.ucar.edu/wrf/src/wps_files/geog_high_res_mandatory.tar.gz tar -xf geog_high_res_mandatory.tar.gz Edit the geog_data_path in namelist.wps Point the directory of high resolution data to WRF_Resources in the nameslist.wps . To do that, you will first have to obtain the path to WPS_GEOG. cd WPS_GEOG pwd Copy the path and go to the directory /shared/scratch/WPS/WRF_Resources/WPS_GEOG/ where your WPS is compiled and edit the namelist.wps. cd /shared/scratch/WPS/ nano namelist.wps Under the &geogrid section, edit the geog_data_path and save it. Edit the &geogrid section in namelist.wps Before executing geogrid.exe, edit the information in the namelist according to your case. Read the description for each listed variables in the namelist, as well as best practice here. We will be using the information provided in the Resources tab in this tutorial. Export the LD_LIBRARY_PATH and run the geogrid.exe. Make sure you had load the intel-oneapi-compilers and intel-oneapi-mpi using spack to avoid issue missing libiomp5.so. export LD_LIBRARY_PATH=$(spack location -i netcdf-fortran%intel)/lib/ spack load intel-oneapi-compilers spack load intel-oneapi-mpi ./geogrid.exe If your geogrid runs successfully, the following output will be printed: Successful completion of geogrid.","title":"GEOGRID"},{"location":"geogrid/#geogrid","text":"","title":"Geogrid"},{"location":"geogrid/#preliminary","text":"Before we start executing geogrid.exe, ungrib.exe and metgrid.exe, create a folder named WRF_Resources to put all the required input data for WPS. We will download all the required data under the /scratch folder. cd /shared/scratch mkdir WRF_Resources cd WRF_Resources","title":"Preliminary"},{"location":"geogrid/#download-the-geogrid-data","text":"You can download the mandatory high resolution data from the official website of UCAR: Download data for the input of geogrid.There are two types of data sets available, each made up of the hightest and lowest resolution of each mandatory field. You may read the details on the official website to decide on the data that suits your need. In this case, we will be using the highest resolution of each mandatory field. wget https://www2.mmm.ucar.edu/wrf/src/wps_files/geog_high_res_mandatory.tar.gz tar -xf geog_high_res_mandatory.tar.gz","title":"Download the geogrid data"},{"location":"geogrid/#edit-the-geog_data_path-in-namelistwps","text":"Point the directory of high resolution data to WRF_Resources in the nameslist.wps . To do that, you will first have to obtain the path to WPS_GEOG. cd WPS_GEOG pwd Copy the path and go to the directory /shared/scratch/WPS/WRF_Resources/WPS_GEOG/ where your WPS is compiled and edit the namelist.wps. cd /shared/scratch/WPS/ nano namelist.wps Under the &geogrid section, edit the geog_data_path and save it.","title":"Edit the geog_data_path in namelist.wps"},{"location":"geogrid/#edit-the-geogrid-section-in-namelistwps","text":"Before executing geogrid.exe, edit the information in the namelist according to your case. Read the description for each listed variables in the namelist, as well as best practice here. We will be using the information provided in the Resources tab in this tutorial. Export the LD_LIBRARY_PATH and run the geogrid.exe. Make sure you had load the intel-oneapi-compilers and intel-oneapi-mpi using spack to avoid issue missing libiomp5.so. export LD_LIBRARY_PATH=$(spack location -i netcdf-fortran%intel)/lib/ spack load intel-oneapi-compilers spack load intel-oneapi-mpi ./geogrid.exe If your geogrid runs successfully, the following output will be printed: Successful completion of geogrid.","title":"Edit the &amp;geogrid section in namelist.wps"},{"location":"intel_compiler/","text":"Intel Compiler Installation spack install --no-cache intel-oneapi-compilers@2022.0.2 Check that the compiler is installed. spack find Load the installed compiler onto Spack so that it recognizes it as a compiler. spack load intel-oneapi-compilers spack compiler find spack unload Then, check the spack compilers command. You should see the Intel compiler that you just installed. spack compilers Install Intel MPI Install Intel MPI using the compiler you just installed. spack install --no-cache intel-oneapi-mpi+external-libfabric%intel","title":"Intel Compiler Installation"},{"location":"intel_compiler/#intel-compiler-installation","text":"spack install --no-cache intel-oneapi-compilers@2022.0.2 Check that the compiler is installed. spack find Load the installed compiler onto Spack so that it recognizes it as a compiler. spack load intel-oneapi-compilers spack compiler find spack unload Then, check the spack compilers command. You should see the Intel compiler that you just installed. spack compilers","title":"Intel Compiler Installation"},{"location":"intel_compiler/#install-intel-mpi","text":"Install Intel MPI using the compiler you just installed. spack install --no-cache intel-oneapi-mpi+external-libfabric%intel","title":"Install Intel MPI"},{"location":"metgrid/","text":"METGRID Export the library path export LD_LIBRARY_PATH=/shared/spack/opt/spack/linux-amzn2-zen2/intel-2021.5.0/netcdf-fortran-4.5.4-a3txw6pecfmvci7zgwpr3p7nzlqt2k2m/lib Load the compilers spack load intel-oneapi-compilers spack load intel-oneapi-mpi Run metgrid ./metgrid.exe","title":"METGRID"},{"location":"metgrid/#metgrid","text":"","title":"METGRID"},{"location":"metgrid/#export-the-library-path","text":"export LD_LIBRARY_PATH=/shared/spack/opt/spack/linux-amzn2-zen2/intel-2021.5.0/netcdf-fortran-4.5.4-a3txw6pecfmvci7zgwpr3p7nzlqt2k2m/lib","title":"Export the library path"},{"location":"metgrid/#load-the-compilers","text":"spack load intel-oneapi-compilers spack load intel-oneapi-mpi","title":"Load the compilers"},{"location":"metgrid/#run-metgrid","text":"./metgrid.exe","title":"Run metgrid"},{"location":"ncl/","text":"NCL Installation Install NCL Using the binary cache, this should take only around 5 minutes. Once installed, check whether ncl is available. spack install ncl^hdf5@1.8.22 spack load ncl ncl -h Now we set up the NCL X11 window size 1000 x 1000 to view the WRF output later. .hluresfile 1 2 3 4 cat << EOF > $HOME/.hluresfile *windowWorkstationClass*wkWidth : 1000 *windowWorkstationClass*wkHeight : 1000 EOF","title":"NCL Installation"},{"location":"ncl/#ncl-installation","text":"","title":"NCL Installation"},{"location":"ncl/#install-ncl","text":"Using the binary cache, this should take only around 5 minutes. Once installed, check whether ncl is available. spack install ncl^hdf5@1.8.22 spack load ncl ncl -h Now we set up the NCL X11 window size 1000 x 1000 to view the WRF output later. .hluresfile 1 2 3 4 cat << EOF > $HOME/.hluresfile *windowWorkstationClass*wkWidth : 1000 *windowWorkstationClass*wkHeight : 1000 EOF","title":"Install NCL"},{"location":"resources/","text":"Resources namelist.input namelist.wps 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 cat <<EOF > namelist.input &time_control run_days = 0, run_hours = 18, run_minutes = 0, run_seconds = 0, start_year = 2021, start_month = 06, start_day = 29, start_hour = 00, start_minute = 00, start_second = 00, end_year = 2021, end_month = 06, end_day = 29, end_hour = 18, end_minute = 00, end_second = 00, interval_seconds = 21600 input_from_file = .true., history_interval = 60, frames_per_outfile = 1, restart = .false., restart_interval = 10000, io_form_history = 2, io_form_restart = 2, io_form_input = 2, io_form_boundary = 2, / &domains perturb_input = .false. time_step = 10, time_step_fract_num = 0, time_step_fract_den = 1, max_dom = 1, s_we = 1, e_we = 100, e_sn = 127, e_vert = 45, dx = 15000, dy = 15000, grid_id = 1, parent_id = 1, i_parent_start = 1, j_parent_start = 1, parent_grid_ratio = 1, parent_time_step_ratio = 1, feedback = 1, smooth_option = 0 num_metgrid_levels = 34, num_metgrid_soil_levels = 4, / &physics physics_suite = 'tropical' mp_physics = 6, ra_lw_physics = 4, ra_sw_physics = 4, radt = 30, sf_sfclay_physics = 91, sf_surface_physics = 2, bl_pbl_physics = 1, bldt = 0, cu_physics = 16, cudt = 0, isfflx = 1, ifsnow = 0, icloud = 1, surface_input_source = 1, num_soil_layers = 1, maxiens = 1, maxens = 1, maxens2 = 1, maxens3 = 1, ensdim = 1, / &dynamics w_damping = 0, diff_opt = 2, km_opt = 4, khdif = 0, kvdif = 0, non_hydrostatic = .true., moist_adv_opt = 1, scalar_adv_opt = 1, use_baseparam_fr_nml = .t. / &bdy_control spec_bdy_width = 5, spec_zone = 1, relax_zone = 4, specified = .true., nested = .false., / &namelist_quilt nio_tasks_per_group = 0, nio_groups = 1, / EOF cat <<EOF > namelist.wps &share wrf_core = 'ARW', max_dom = 1, start_date = '2021-06-29_00:00:00','2021-06-29_00:00:00', end_date = '2021-06-29_18:00:00','2021-06-29_18:00:00', interval_seconds = 21600 / &geogrid parent_id = 1, 1, parent_grid_ratio = 1, 3, i_parent_start = 1, 53, j_parent_start = 1, 25, e_we = 100, 220, e_sn = 127, 214, geog_data_res = 'default','default', dx = 15000, dy = 15000, map_proj = 'mercator', ref_lat = 5.464, ref_lon = 100.289, truelat1 = 5.464, truelat2 = 0, stand_lon = 100.261, geog_data_path = '/shared/scratch/WRF_Resources/WPS_GEOG/' / &ungrib out_format = 'WPS', prefix = 'FILE', / &metgrid fg_name = 'FILE' / EOF","title":"Resources"},{"location":"resources/#resources","text":"namelist.input namelist.wps 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 cat <<EOF > namelist.input &time_control run_days = 0, run_hours = 18, run_minutes = 0, run_seconds = 0, start_year = 2021, start_month = 06, start_day = 29, start_hour = 00, start_minute = 00, start_second = 00, end_year = 2021, end_month = 06, end_day = 29, end_hour = 18, end_minute = 00, end_second = 00, interval_seconds = 21600 input_from_file = .true., history_interval = 60, frames_per_outfile = 1, restart = .false., restart_interval = 10000, io_form_history = 2, io_form_restart = 2, io_form_input = 2, io_form_boundary = 2, / &domains perturb_input = .false. time_step = 10, time_step_fract_num = 0, time_step_fract_den = 1, max_dom = 1, s_we = 1, e_we = 100, e_sn = 127, e_vert = 45, dx = 15000, dy = 15000, grid_id = 1, parent_id = 1, i_parent_start = 1, j_parent_start = 1, parent_grid_ratio = 1, parent_time_step_ratio = 1, feedback = 1, smooth_option = 0 num_metgrid_levels = 34, num_metgrid_soil_levels = 4, / &physics physics_suite = 'tropical' mp_physics = 6, ra_lw_physics = 4, ra_sw_physics = 4, radt = 30, sf_sfclay_physics = 91, sf_surface_physics = 2, bl_pbl_physics = 1, bldt = 0, cu_physics = 16, cudt = 0, isfflx = 1, ifsnow = 0, icloud = 1, surface_input_source = 1, num_soil_layers = 1, maxiens = 1, maxens = 1, maxens2 = 1, maxens3 = 1, ensdim = 1, / &dynamics w_damping = 0, diff_opt = 2, km_opt = 4, khdif = 0, kvdif = 0, non_hydrostatic = .true., moist_adv_opt = 1, scalar_adv_opt = 1, use_baseparam_fr_nml = .t. / &bdy_control spec_bdy_width = 5, spec_zone = 1, relax_zone = 4, specified = .true., nested = .false., / &namelist_quilt nio_tasks_per_group = 0, nio_groups = 1, / EOF cat <<EOF > namelist.wps &share wrf_core = 'ARW', max_dom = 1, start_date = '2021-06-29_00:00:00','2021-06-29_00:00:00', end_date = '2021-06-29_18:00:00','2021-06-29_18:00:00', interval_seconds = 21600 / &geogrid parent_id = 1, 1, parent_grid_ratio = 1, 3, i_parent_start = 1, 53, j_parent_start = 1, 25, e_we = 100, 220, e_sn = 127, 214, geog_data_res = 'default','default', dx = 15000, dy = 15000, map_proj = 'mercator', ref_lat = 5.464, ref_lon = 100.289, truelat1 = 5.464, truelat2 = 0, stand_lon = 100.261, geog_data_path = '/shared/scratch/WRF_Resources/WPS_GEOG/' / &ungrib out_format = 'WPS', prefix = 'FILE', / &metgrid fg_name = 'FILE' / EOF","title":"Resources"},{"location":"run_wrf/","text":"Running WRF Congratulations on your successful running of WPS \ud83e\udd73 Now you are just one step away! Copy the WRF file in the newly created WRF folder in /scratch . Go into the WRF/run directory and link met_em* file. cd /shared/scratch mkdir WRF cd WRF cp -a $(spack location -i wrf%intel)/. /shared/scratch/WRF cd run ln -sf /shared/scratch/WPS/met_em* . ls -alh Edit the namelist.input and make necessary changes or simply overwrite the namelist from the Resources tab. The details including &time_control and &domains has to be the same as defined in the namelist.wps .You may change the &physics option here too. Refer to UCAR Website for the best practices. nano namelist.input Execute the real program. Load the WRF package before you run. spack load wrf mpirun -np 1 ./real.exe Check the rsl.error files to ensure that the run was successful. To do that, you can use the tail command. Usually, the rsl.error0000 will contain the most information.This indicate the successful run of real.exe , real_em: SUCCESS COMPLETE REAL_EM INIT . tail rsl.error.0000 Now you are ready to run the wrf.exe . We can use the slurm to submit and distribute the tasks for us. In order for the slurm to run wrf, make sure that it is in the same directory where your wrf.exe is located. Kindly amend the information according to your need. slurm-wrf-penang.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 cat <<EOF > slurm-wrf-penang.sh #!/bin/bash #SBATCH --job-name=WRF #SBATCH --output=penang-%j.out #SBATCH --nodes=2 #SBATCH --ntasks-per-node=16 #SBATCH --exclusive spack load wrf set -x wrf_exe=$(spack location -i wrf)/run/wrf.exe ulimit -s unlimited ulimit -a export OMP_NUM_THREADS=6 export FI_PROVIDER=efa export I_MPI_FABRICS=ofi export I_MPI_OFI_LIBRARY_INTERNAL=0 export I_MPI_OFI_PROVIDER=efa export I_MPI_PIN_DOMAIN=omp export KMP_AFFINITY=compact export I_MPI_DEBUG=4 set +x module load intelmpi set -x time mpirun -np $SLURM_NTASKS --ppn $SLURM_NTASKS_PER_NODE $wrf_exe EOF Submit the slurm job. You can check the run by squeue . spack load intel-oneapi-compilers spack load intel-oneapi-mpi spack load wrf sbatch slurm-wrf-penang.sh Check the rsl.out.0000 file to see if the run was successful. You will see a successful message printed at the end of the file. tail rsl.out.0000","title":"Running WRF"},{"location":"run_wrf/#running-wrf","text":"Congratulations on your successful running of WPS \ud83e\udd73 Now you are just one step away! Copy the WRF file in the newly created WRF folder in /scratch . Go into the WRF/run directory and link met_em* file. cd /shared/scratch mkdir WRF cd WRF cp -a $(spack location -i wrf%intel)/. /shared/scratch/WRF cd run ln -sf /shared/scratch/WPS/met_em* . ls -alh Edit the namelist.input and make necessary changes or simply overwrite the namelist from the Resources tab. The details including &time_control and &domains has to be the same as defined in the namelist.wps .You may change the &physics option here too. Refer to UCAR Website for the best practices. nano namelist.input Execute the real program. Load the WRF package before you run. spack load wrf mpirun -np 1 ./real.exe Check the rsl.error files to ensure that the run was successful. To do that, you can use the tail command. Usually, the rsl.error0000 will contain the most information.This indicate the successful run of real.exe , real_em: SUCCESS COMPLETE REAL_EM INIT . tail rsl.error.0000 Now you are ready to run the wrf.exe . We can use the slurm to submit and distribute the tasks for us. In order for the slurm to run wrf, make sure that it is in the same directory where your wrf.exe is located. Kindly amend the information according to your need. slurm-wrf-penang.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 cat <<EOF > slurm-wrf-penang.sh #!/bin/bash #SBATCH --job-name=WRF #SBATCH --output=penang-%j.out #SBATCH --nodes=2 #SBATCH --ntasks-per-node=16 #SBATCH --exclusive spack load wrf set -x wrf_exe=$(spack location -i wrf)/run/wrf.exe ulimit -s unlimited ulimit -a export OMP_NUM_THREADS=6 export FI_PROVIDER=efa export I_MPI_FABRICS=ofi export I_MPI_OFI_LIBRARY_INTERNAL=0 export I_MPI_OFI_PROVIDER=efa export I_MPI_PIN_DOMAIN=omp export KMP_AFFINITY=compact export I_MPI_DEBUG=4 set +x module load intelmpi set -x time mpirun -np $SLURM_NTASKS --ppn $SLURM_NTASKS_PER_NODE $wrf_exe EOF Submit the slurm job. You can check the run by squeue . spack load intel-oneapi-compilers spack load intel-oneapi-mpi spack load wrf sbatch slurm-wrf-penang.sh Check the rsl.out.0000 file to see if the run was successful. You will see a successful message printed at the end of the file. tail rsl.out.0000","title":"Running WRF"},{"location":"spack/","text":"Spack Installation Verify the Cluster It is always good to verify the cluster information (sinfo, filesystems, etc.) before you start the installation. Check the file systems through this command. df -h Check the basic Slurm information. You will notice that all 10 nodes is in idle state because there is no ongoing activity submitted now. sinfo Spack Installation Now, we define the variable SPACK_ROOT and download Spack Version 0.18.0 under the /spack folder created. export SPACK_ROOT=/shared/spack mkdir -p $SPACK_ROOT git clone -b v0.18.0 -c feature.manyFiles=true https://github.com/spack/spack $SPACK_ROOT Define the .bashrc file and source it to run the preconfigured spack everytime you open a new terminal. echo \"export SPACK_ROOT=/shared/spack\" >> $HOME/.bashrc echo \"source \\$SPACK_ROOT/share/spack/setup-env.sh\" >> $HOME/.bashrc source $HOME/.bashrc To check your Spack Version, type this or if you want to know more about the Spack you installed, try spack -h . spack -V Notice that the version installed is 0.18.0 . Patchelf Installation Patchelf is a small utility to modify the dynamic linker and RPATH of ELF executables. spack install patchelf Check the patchelf location and see if it is properly installed. spack find spack load patchelf which patchelf Spack build cache Install prerequisites pip3 install botocore==1.23.46 boto3==1.20.46 Add the mirror to the binary cache spack mirror add aws-hpc-weather s3://aws-hpc-weather/spack/ spack buildcache keys --install --trust --force Adding parallel cluster's softwares to the Spack so that it recognises that these packages are installed. Copy and paste the following into your CLI. packages.yaml 1 2 3 4 5 6 7 8 9 cat << EOF > $SPACK_ROOT/etc/spack/packages.yaml packages: libfabric: variants: fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm externals: - spec: libfabric@1.13.2 fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm prefix: /opt/amazon/efa buildable: False EOF","title":"Spack Installation"},{"location":"spack/#spack-installation","text":"","title":"Spack Installation"},{"location":"spack/#verify-the-cluster","text":"It is always good to verify the cluster information (sinfo, filesystems, etc.) before you start the installation. Check the file systems through this command. df -h Check the basic Slurm information. You will notice that all 10 nodes is in idle state because there is no ongoing activity submitted now. sinfo","title":"Verify the Cluster"},{"location":"spack/#spack-installation_1","text":"Now, we define the variable SPACK_ROOT and download Spack Version 0.18.0 under the /spack folder created. export SPACK_ROOT=/shared/spack mkdir -p $SPACK_ROOT git clone -b v0.18.0 -c feature.manyFiles=true https://github.com/spack/spack $SPACK_ROOT Define the .bashrc file and source it to run the preconfigured spack everytime you open a new terminal. echo \"export SPACK_ROOT=/shared/spack\" >> $HOME/.bashrc echo \"source \\$SPACK_ROOT/share/spack/setup-env.sh\" >> $HOME/.bashrc source $HOME/.bashrc To check your Spack Version, type this or if you want to know more about the Spack you installed, try spack -h . spack -V Notice that the version installed is 0.18.0 . Patchelf Installation Patchelf is a small utility to modify the dynamic linker and RPATH of ELF executables. spack install patchelf Check the patchelf location and see if it is properly installed. spack find spack load patchelf which patchelf Spack build cache Install prerequisites pip3 install botocore==1.23.46 boto3==1.20.46 Add the mirror to the binary cache spack mirror add aws-hpc-weather s3://aws-hpc-weather/spack/ spack buildcache keys --install --trust --force Adding parallel cluster's softwares to the Spack so that it recognises that these packages are installed. Copy and paste the following into your CLI. packages.yaml 1 2 3 4 5 6 7 8 9 cat << EOF > $SPACK_ROOT/etc/spack/packages.yaml packages: libfabric: variants: fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm externals: - spec: libfabric@1.13.2 fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm prefix: /opt/amazon/efa buildable: False EOF","title":"Spack Installation"},{"location":"test_wrf/","text":"Testing WRF To check if the WRF installed can run properly, input data used are 12-km CONUS input. These are used to run the WRF executable (wrf.exe) to simulate atmospheric events that took place during the Pre-Thanksgiving Winter Storm of 2019 . The model domain includes the entire Continental United States (CONUS), using 12-km grid spacing, which means that each grid point is 12x12 km. The full domain contains 425 x 300 grid points. After running the WRF model, post-processing will allow visualization of atmospheric variables available in the output (e.g., temperature, wind speed, pressure). Load WRF Load the package WRF in spack. spack load wrf Find the executable for WRF . You should expect four executables: real.exe , wrf.exe , ndown.exe and tc.exe for a successful compilation. ls `spack location -i wrf`/run/ Create scratch folder under /shared . cd /shared mkdir scratch cd scratch Download the file. curl -O https://www2.mmm.ucar.edu/wrf/OnLineTutorial/wrf_cloud/wrf_simulation_CONUS12km.tar.gz tar -xzf wrf_simulation_CONUS12km.tar.gz Next we\u2019ll prepare the data for a run by copying in the relevant files from our WRF install.It is safe to ignore the error message ln: failed to create symbolic link \u2018./namelist.input\u2019: File exists since there is existing namelist.input file that contains all the necessary information for the run. cd conus_12km WRF_ROOT=$(spack location -i wrf%intel)/test/em_real/ ln -s $WRF_ROOT* . Let's create a Slurm job submission script in the conus_12km. This file specifies the job submission script as follows: Resource No. Description --nodes 2 Using 2x Hpc6a.48xl instances --ntasks-per-node 16 Each node has 16 MPI processes OMP_NUM_THREADS 6 Each node has 6 OpenMP threads slurm-wrf-conus12km.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 cat <<EOF > slurm-wrf-conus12km.sh #!/bin/bash #!/bin/bash #SBATCH --job-name=WRF #SBATCH --output=conus-%j.out #SBATCH --nodes=2 #SBATCH --ntasks-per-node=16 #SBATCH --exclusive spack load wrf set -x wrf_exe=\\$(spack location -i wrf)/run/wrf.exe ulimit -s unlimited ulimit -a export OMP_NUM_THREADS=6 export FI_PROVIDER=efa export I_MPI_FABRICS=ofi export I_MPI_OFI_LIBRARY_INTERNAL=0 export I_MPI_OFI_PROVIDER=efa export I_MPI_PIN_DOMAIN=omp export KMP_AFFINITY=compact export I_MPI_DEBUG=4 set +x module load intelmpi set -x time mpirun -np \\$SLURM_NTASKS --ppn \\$SLURM_NTASKS_PER_NODE \\$wrf_exe EOF Submit the run using the Slurm Workload Manager. sbatch slurm-wrf-conus12km.sh DCV Open a NICE DCV session through PCluster Manager.Once you are connected over NICE DCV, open the terminal and execute the following. cd /shared/scratch/conus_12km spack load ncl ncl ncl_scripts/surface.ncl You can also generate a vertical profile of relative humidity (%) and temperature (K). cd /shared/scratch/conus_12km spack load ncl ncl ncl_scripts/vert_crossSection.ncl","title":"Testing WRF"},{"location":"test_wrf/#testing-wrf","text":"To check if the WRF installed can run properly, input data used are 12-km CONUS input. These are used to run the WRF executable (wrf.exe) to simulate atmospheric events that took place during the Pre-Thanksgiving Winter Storm of 2019 . The model domain includes the entire Continental United States (CONUS), using 12-km grid spacing, which means that each grid point is 12x12 km. The full domain contains 425 x 300 grid points. After running the WRF model, post-processing will allow visualization of atmospheric variables available in the output (e.g., temperature, wind speed, pressure).","title":"Testing WRF"},{"location":"test_wrf/#load-wrf","text":"Load the package WRF in spack. spack load wrf Find the executable for WRF . You should expect four executables: real.exe , wrf.exe , ndown.exe and tc.exe for a successful compilation. ls `spack location -i wrf`/run/ Create scratch folder under /shared . cd /shared mkdir scratch cd scratch Download the file. curl -O https://www2.mmm.ucar.edu/wrf/OnLineTutorial/wrf_cloud/wrf_simulation_CONUS12km.tar.gz tar -xzf wrf_simulation_CONUS12km.tar.gz Next we\u2019ll prepare the data for a run by copying in the relevant files from our WRF install.It is safe to ignore the error message ln: failed to create symbolic link \u2018./namelist.input\u2019: File exists since there is existing namelist.input file that contains all the necessary information for the run. cd conus_12km WRF_ROOT=$(spack location -i wrf%intel)/test/em_real/ ln -s $WRF_ROOT* . Let's create a Slurm job submission script in the conus_12km. This file specifies the job submission script as follows: Resource No. Description --nodes 2 Using 2x Hpc6a.48xl instances --ntasks-per-node 16 Each node has 16 MPI processes OMP_NUM_THREADS 6 Each node has 6 OpenMP threads slurm-wrf-conus12km.sh 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 cat <<EOF > slurm-wrf-conus12km.sh #!/bin/bash #!/bin/bash #SBATCH --job-name=WRF #SBATCH --output=conus-%j.out #SBATCH --nodes=2 #SBATCH --ntasks-per-node=16 #SBATCH --exclusive spack load wrf set -x wrf_exe=\\$(spack location -i wrf)/run/wrf.exe ulimit -s unlimited ulimit -a export OMP_NUM_THREADS=6 export FI_PROVIDER=efa export I_MPI_FABRICS=ofi export I_MPI_OFI_LIBRARY_INTERNAL=0 export I_MPI_OFI_PROVIDER=efa export I_MPI_PIN_DOMAIN=omp export KMP_AFFINITY=compact export I_MPI_DEBUG=4 set +x module load intelmpi set -x time mpirun -np \\$SLURM_NTASKS --ppn \\$SLURM_NTASKS_PER_NODE \\$wrf_exe EOF Submit the run using the Slurm Workload Manager. sbatch slurm-wrf-conus12km.sh","title":"Load WRF"},{"location":"test_wrf/#dcv","text":"Open a NICE DCV session through PCluster Manager.Once you are connected over NICE DCV, open the terminal and execute the following. cd /shared/scratch/conus_12km spack load ncl ncl ncl_scripts/surface.ncl You can also generate a vertical profile of relative humidity (%) and temperature (K). cd /shared/scratch/conus_12km spack load ncl ncl ncl_scripts/vert_crossSection.ncl","title":"DCV"},{"location":"ungrib/","text":"UNGRIB Create GFS data under /WRF_Resources that you just created. cd /shared/scratch/WRF_Resources mkdir GFS_Data cd GFS_Data Before you start using the python script, check the website Research Data Archive website to see if the data sets for your interested dates are available. Create a python script to download the GFS Data. Make necessary changes on the dates and hours.This python script downloads 4 data on 20210629 with 6 hours interval. Fill in YOUR EMAIL (line 47) to avoid bad authentication. The alternatives would be download the python script and upload through DCV. download_20210629.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 cat << EOF > download_20210629 . py #!/usr/bin/env python ################################################################# # Python Script to retrieve 4 online Data files of 'ds084.1', # total 2.06G. This script uses 'requests' to download data. # # Highlight this script by Select All, Copy and Paste it into a file; # make the file executable and run it on command line. # # You need pass in your password as a parameter to execute # this script; or you can set an environment variable RDAPSWD # if your Operating System supports it. # # Contact rdahelp@ucar.edu (RDA help desk) for further assistance. ################################################################# import sys , os import requests def check_file_status ( filepath , filesize ): sys . stdout . write ( ' \\r ' ) sys . stdout . flush () size = int ( os . stat ( filepath ) . st_size ) percent_complete = ( size / filesize ) * 100 sys . stdout . write ( ' %.3f %s ' % ( percent_complete , '% Completed' )) sys . stdout . flush () # Try to get password if len ( sys . argv ) < 2 and not 'RDAPSWD' in os . environ : try : import getpass input = getpass . getpass except : try : input = raw_input except : pass pswd = input ( 'Password: ' ) else : try : pswd = sys . argv [ 1 ] except : pswd = os . environ [ 'RDAPSWD' ] url = 'https://rda.ucar.edu/cgi-bin/login' values = { 'email' : 'YOUR EMAIL' , 'passwd' : pswd , 'action' : 'login' } # Authenticate ret = requests . post ( url , data = values ) if ret . status_code != 200 : print ( 'Bad Authentication' ) print ( ret . text ) exit ( 1 ) dspath = 'https://rda.ucar.edu/data/ds084.1/' filelist = [ '2021/20210629/gfs.0p25.2021062900.f000.grib2' , '2021/20210629/gfs.0p25.2021062906.f000.grib2' , '2021/20210629/gfs.0p25.2021062912.f000.grib2' , '2021/20210629/gfs.0p25.2021062918.f000.grib2' ] for file in filelist : filename = dspath + file file_base = os . path . basename ( file ) print ( 'Downloading' , file_base ) req = requests . get ( filename , cookies = ret . cookies , allow_redirects = True , stream = True ) filesize = int ( req . headers [ 'Content-length' ]) with open ( file_base , 'wb' ) as outfile : chunk_size = 1048576 for chunk in req . iter_content ( chunk_size = chunk_size ): outfile . write ( chunk ) if chunk_size < filesize : check_file_status ( file_base , filesize ) check_file_status ( file_base , filesize ) print () EOF Execute the python script to download the data. You must be a registered user on the NCAR website because you will be asked for password before the permission to download the data is granted. pip3 install requests python3 download_20210629.py Obtain the path to your GFS_Data where in this case, it is /shared/scratch/WPS . Go back to the directory where your WPS is compiled and create the symbolic link to the GFS data that you just downloaded. You should expect four GRIBFILE with same prefix but different labels behind, AAA, AAB, AAC and AAD. cd /shared/scratch/WPS ./link_grib.csh /shared/scratch/WPS/WRF_Resources/GFS_Data/gfs.* Create the symbolic link to the Vtable. ln -sf ungrib/Variable_Tables/Vtable.GFS Vtable ls -alh Now, it is time to edit the namelist.wps . Below are the few things that should be amended according to the GFS data. start date end date the interval seconds that desribe the interval between your GFS data nano namelist.wps Increase your stack limit as the superuser. sudo -s ulimit -s unlimited Export the library path. export LD_LIBRARY_PATH=/shared/spack/opt/spack/linux-amzn2-zen2/intel-2021.5.0/jasper-2.0.31-skcu73p6hnlgov6teechaq6muly2xrez/lib64/:\\$LD_LIBRARY_PATH Run the ungrib. ./ungrib.exe You will see the successful output printed out at the completion of ungrib.exe and four output with prefix FILE are created. Remember to exit the superuser by typing exit. exit","title":"UNGRIB"},{"location":"ungrib/#ungrib","text":"Create GFS data under /WRF_Resources that you just created. cd /shared/scratch/WRF_Resources mkdir GFS_Data cd GFS_Data Before you start using the python script, check the website Research Data Archive website to see if the data sets for your interested dates are available. Create a python script to download the GFS Data. Make necessary changes on the dates and hours.This python script downloads 4 data on 20210629 with 6 hours interval. Fill in YOUR EMAIL (line 47) to avoid bad authentication. The alternatives would be download the python script and upload through DCV. download_20210629.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 cat << EOF > download_20210629 . py #!/usr/bin/env python ################################################################# # Python Script to retrieve 4 online Data files of 'ds084.1', # total 2.06G. This script uses 'requests' to download data. # # Highlight this script by Select All, Copy and Paste it into a file; # make the file executable and run it on command line. # # You need pass in your password as a parameter to execute # this script; or you can set an environment variable RDAPSWD # if your Operating System supports it. # # Contact rdahelp@ucar.edu (RDA help desk) for further assistance. ################################################################# import sys , os import requests def check_file_status ( filepath , filesize ): sys . stdout . write ( ' \\r ' ) sys . stdout . flush () size = int ( os . stat ( filepath ) . st_size ) percent_complete = ( size / filesize ) * 100 sys . stdout . write ( ' %.3f %s ' % ( percent_complete , '% Completed' )) sys . stdout . flush () # Try to get password if len ( sys . argv ) < 2 and not 'RDAPSWD' in os . environ : try : import getpass input = getpass . getpass except : try : input = raw_input except : pass pswd = input ( 'Password: ' ) else : try : pswd = sys . argv [ 1 ] except : pswd = os . environ [ 'RDAPSWD' ] url = 'https://rda.ucar.edu/cgi-bin/login' values = { 'email' : 'YOUR EMAIL' , 'passwd' : pswd , 'action' : 'login' } # Authenticate ret = requests . post ( url , data = values ) if ret . status_code != 200 : print ( 'Bad Authentication' ) print ( ret . text ) exit ( 1 ) dspath = 'https://rda.ucar.edu/data/ds084.1/' filelist = [ '2021/20210629/gfs.0p25.2021062900.f000.grib2' , '2021/20210629/gfs.0p25.2021062906.f000.grib2' , '2021/20210629/gfs.0p25.2021062912.f000.grib2' , '2021/20210629/gfs.0p25.2021062918.f000.grib2' ] for file in filelist : filename = dspath + file file_base = os . path . basename ( file ) print ( 'Downloading' , file_base ) req = requests . get ( filename , cookies = ret . cookies , allow_redirects = True , stream = True ) filesize = int ( req . headers [ 'Content-length' ]) with open ( file_base , 'wb' ) as outfile : chunk_size = 1048576 for chunk in req . iter_content ( chunk_size = chunk_size ): outfile . write ( chunk ) if chunk_size < filesize : check_file_status ( file_base , filesize ) check_file_status ( file_base , filesize ) print () EOF Execute the python script to download the data. You must be a registered user on the NCAR website because you will be asked for password before the permission to download the data is granted. pip3 install requests python3 download_20210629.py Obtain the path to your GFS_Data where in this case, it is /shared/scratch/WPS . Go back to the directory where your WPS is compiled and create the symbolic link to the GFS data that you just downloaded. You should expect four GRIBFILE with same prefix but different labels behind, AAA, AAB, AAC and AAD. cd /shared/scratch/WPS ./link_grib.csh /shared/scratch/WPS/WRF_Resources/GFS_Data/gfs.* Create the symbolic link to the Vtable. ln -sf ungrib/Variable_Tables/Vtable.GFS Vtable ls -alh Now, it is time to edit the namelist.wps . Below are the few things that should be amended according to the GFS data. start date end date the interval seconds that desribe the interval between your GFS data nano namelist.wps Increase your stack limit as the superuser. sudo -s ulimit -s unlimited Export the library path. export LD_LIBRARY_PATH=/shared/spack/opt/spack/linux-amzn2-zen2/intel-2021.5.0/jasper-2.0.31-skcu73p6hnlgov6teechaq6muly2xrez/lib64/:\\$LD_LIBRARY_PATH Run the ungrib. ./ungrib.exe You will see the successful output printed out at the completion of ungrib.exe and four output with prefix FILE are created. Remember to exit the superuser by typing exit. exit","title":"UNGRIB"},{"location":"wps/","text":"WPS Installation wps-install.sbatch 1 2 3 4 5 6 7 cat <<EOF > wps-install.sbatch #!/bin/bash #SBATCH -N 1 #SBATCH --exclusive spack install -j 1 wps%intel ^intel-oneapi-mpi+external-libfabric EOF sbatch wps-install.sbatch Checking the Clusters squeue cat tail cat slurm-<jobID>.out tail slurm-<jobID>.out spack find","title":"WPS Installation"},{"location":"wps/#wps-installation","text":"wps-install.sbatch 1 2 3 4 5 6 7 cat <<EOF > wps-install.sbatch #!/bin/bash #SBATCH -N 1 #SBATCH --exclusive spack install -j 1 wps%intel ^intel-oneapi-mpi+external-libfabric EOF sbatch wps-install.sbatch","title":"WPS Installation"},{"location":"wps/#checking-the-clusters","text":"squeue cat tail cat slurm-<jobID>.out tail slurm-<jobID>.out spack find","title":"Checking the Clusters"},{"location":"wps_configuration/","text":"WPS Configuration Create a soft link for the WPS Load the compilers before you start doing anything. spack load intel-oneapi-compilers spack load intel-oneapi-mpi COPY INSTEAD OF USING THE SOFT LINK cd /shared/scratch mkdir WPS cp -a $(spack location -i wps%intel)/. WPS/ cd WPS ls Jasper Installation spack install jasper@2.0.31%intel Check if the jasper version 2.0.31 is installed. spack find Export export JASPERLIB=$(spack location -i jasper@2.0.31%intel)/lib64 export JASPERINC=$(spack location -i jasper@2.0.31%intel)/include export WRF_DIR=$(spack location -i wrf%intel) export NETCDFINC=$(spack location -i netcdf-fortran%intel)/include export NETCDFLIB=$(spack location -i netcdf-fortran%intel)/lib export NETCDF=$(spack location -i netcdf-fortran%intel) export NETCDFF=$(spack location -i netcdf-fortran%intel) Check if the variables are imported correctly. echo $NETCDFINC echo $NETCDFLIB Then, copy the contents of lib and include of netcdf-c to netcdf-fortran of the intel compilers. cp -a $(spack location -i netcdf-c%intel)/include/. $(spack location -i netcdf-fortran%intel)/include/ cp -a $(spack location -i netcdf-c%intel)/lib/. $(spack location -i netcdf-fortran%intel)/lib/ Edit configure.wps File ./configure If you did not set the path for NETCDFINC , type Y then paste the NETCDFINC path as given above. nano configure.wps Look for WRF_LIB and add -qopenmp after -lnetcdff -lnetcdf then save. Compile ./compile > log.compile Clean Note to reset WPS settings! ./clean -a","title":"WPS Configuration"},{"location":"wps_configuration/#wps-configuration","text":"","title":"WPS Configuration"},{"location":"wps_configuration/#create-a-soft-link-for-the-wps","text":"Load the compilers before you start doing anything. spack load intel-oneapi-compilers spack load intel-oneapi-mpi COPY INSTEAD OF USING THE SOFT LINK cd /shared/scratch mkdir WPS cp -a $(spack location -i wps%intel)/. WPS/ cd WPS ls","title":"Create a soft link for the WPS"},{"location":"wps_configuration/#jasper-installation","text":"spack install jasper@2.0.31%intel Check if the jasper version 2.0.31 is installed. spack find","title":"Jasper Installation"},{"location":"wps_configuration/#export","text":"export JASPERLIB=$(spack location -i jasper@2.0.31%intel)/lib64 export JASPERINC=$(spack location -i jasper@2.0.31%intel)/include export WRF_DIR=$(spack location -i wrf%intel) export NETCDFINC=$(spack location -i netcdf-fortran%intel)/include export NETCDFLIB=$(spack location -i netcdf-fortran%intel)/lib export NETCDF=$(spack location -i netcdf-fortran%intel) export NETCDFF=$(spack location -i netcdf-fortran%intel) Check if the variables are imported correctly. echo $NETCDFINC echo $NETCDFLIB Then, copy the contents of lib and include of netcdf-c to netcdf-fortran of the intel compilers. cp -a $(spack location -i netcdf-c%intel)/include/. $(spack location -i netcdf-fortran%intel)/include/ cp -a $(spack location -i netcdf-c%intel)/lib/. $(spack location -i netcdf-fortran%intel)/lib/","title":"Export"},{"location":"wps_configuration/#edit-configurewps-file","text":"./configure If you did not set the path for NETCDFINC , type Y then paste the NETCDFINC path as given above. nano configure.wps Look for WRF_LIB and add -qopenmp after -lnetcdff -lnetcdf then save.","title":"Edit configure.wps File"},{"location":"wps_configuration/#compile","text":"./compile > log.compile","title":"Compile"},{"location":"wps_configuration/#clean","text":"Note to reset WPS settings! ./clean -a","title":"Clean"}]}