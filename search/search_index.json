{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Create Cluster","text":"<p>This tutorial covers from the cluster creation using PClusterManager to  the configuration and compilation of WRF Version 4.3.3 (Weather Research and Forecasting) and WPS version 4.3.1 (WRF Preprocessing System) using Spack.</p>"},{"location":"#create-cluster","title":"Create Cluster","text":"<p>We have to create a computer cluster on AWS to install and run WRF. The links to AWS and PCluster Managers are below. Go to the PCluster link to create the cluster after having a account on AWS.</p> <ul> <li>Link to AWS</li> <li>Link to PCluster Manager</li> </ul> <p></p>"},{"location":"#upload-yml-file","title":"Upload YML file","text":"<p>PCluster Manager makes it easy for us to create and manage the clusters through interface. This is an example configuration. For your first time, you may choose to upload the yml file we have prepared for you and configure to your prefered headnode and shared storage later by updating the yaml file further. In order for AWS to verify your identity, you will need to create your own EC2 Key Pair using Amazon EC2.  </p> YAML File<pre><code>HeadNode:\nInstanceType: c5a.xlarge\nSsh:\nKeyName: PClusterManager\nNetworking:\nSubnetId: subnet-0412e7315562aa1da\nLocalStorage:\nRootVolume:\nVolumeType: gp3\nSize: 50\nIam:\nAdditionalIamPolicies:\n- Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\n- Policy: arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\nDcv:\nEnabled: true\nScheduling:\nScheduler: slurm\nSlurmQueues:\n- Name: queue0\nComputeResources:\n- Name: queue0-hpc6a48xlarge\nMinCount: 0\nMaxCount: 10\nInstanceType: hpc6a.48xlarge\nEfa:\nEnabled: true\nGdrSupport: true\nDisableSimultaneousMultithreading: true\nNetworking:\nSubnetIds:\n- subnet-01a39ae1f7194644a\nPlacementGroup:\nEnabled: true\nComputeSettings:\nLocalStorage:\nRootVolume:\nVolumeType: gp3\nSize: 50\nIam:\nAdditionalIamPolicies:\n- Policy: arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\nRegion: us-east-2\nImage:\nOs: alinux2\nSharedStorage:\n- Name: Ebs0\nStorageType: Ebs\nMountDir: /shared\nEbsSettings:\nVolumeType: gp3\nDeletionPolicy: Delete\nSize: '200'\nEncrypted: false\n</code></pre>"},{"location":"#create-cluster_1","title":"Create Cluster","text":"<ul> <li>Dry Run</li> </ul> <p>You can opt to dry run your cluster before creating it. After dry run, you will see this message printed: <code>Request would have succeeded, but DryRun flag is set</code>.</p> <ul> <li>Create</li> </ul> <p>Creating cluster will take around 15-20 minutes. You may check if you are interested in the steps or current progress of stack events.</p>"},{"location":"#create-in-progress","title":"Create in Progress","text":""},{"location":"#create-complete","title":"Create Complete","text":"<p>Now you should be able to see the cluster <code>WRF</code> in <code>CREATE_COMPLETE</code> status from your PCluster Manager interface. Click on Shell and login to your own AWS account to access the terminal.</p> <p></p>"},{"location":"delete_spack/","title":"Delete existing Spack for fresh installation","text":""},{"location":"delete_spack/#export-variable","title":"Export variable","text":"<p>You can just remove the existing <code>$SPACK_ROOT</code> and install a new Spack environment. For example, if $SPACK_ROOT is <code>/shared/spack</code>, you can do:</p> <pre><code>export SPACK_ROOT=/shared/spack\n</code></pre>"},{"location":"delete_spack/#remove-spack-from-the-system","title":"Remove spack from the system","text":"<pre><code>rm -rf $SPACK_ROOT\n</code></pre>"},{"location":"delete_spack/#download-spack","title":"Download Spack","text":"<p>Now, you can download Spack and go through the steps of <code>Spack Installation</code> again.</p> <pre><code>export SPACK_ROOT=/shared/spack\nmkdir -p $SPACK_ROOT\ngit clone -b v0.18.0 -c feature.manyFiles=true https://github.com/spack/spack $SPACK_ROOT\n</code></pre>"},{"location":"geogrid/","title":"Geogrid","text":""},{"location":"geogrid/#preliminary","title":"Preliminary","text":"<p>Before we start executing <code>geogrid.exe</code>, <code>ungrib.exe</code>, and <code>metgrid.exe</code>, we need to create a folder named <code>WRF_Resources</code> to put all the required input data for WPS. We will download all the required data into the <code>/scratch</code> folder. If you have downloaded them, proceed to edit the <code>&amp;geogrid</code> section in the <code>namelist.wps</code>.</p> <pre><code>cd /shared/scratch\nmkdir WRF_Resources\ncd WRF_Resources\n</code></pre>"},{"location":"geogrid/#download-the-geogrid-data","title":"Download the geogrid data","text":"<p>You can download the mandatory high-resolution data from the official website of UCAR:  Download data for the input of <code>geogrid.exe</code>. There are two types of datasets available, each made up of the highest and lowest resolution of each mandatory field. You may read the details on the official website to decide on the data that suits your needs. In this case, we will be using the highest resolution of each mandatory field.</p> <pre><code>wget https://www2.mmm.ucar.edu/wrf/src/wps_files/geog_high_res_mandatory.tar.gz\ntar -xf geog_high_res_mandatory.tar.gz\n</code></pre>"},{"location":"geogrid/#edit-the-geog_data_path-in-the-namelistwps","title":"Edit the geog_data_path in the namelist.wps","text":"<p>Point the directory of high-resolution data to WRF_Resources in the <code>nameslist.wps</code>. To do that, you must first obtain the path to <code>WPS_GEOG</code>.</p> <pre><code>cd WPS_GEOG\npwd\n</code></pre> <p>Copy the path and go to the directory <code>/shared/scratch/WPS/WRF_Resources/WPS_GEOG/</code> where your WPS was compiled and edit the <code>namelist.wps</code>.</p> <pre><code>cd /shared/scratch/WPS/\nnano namelist.wps\n</code></pre> <p>Under the <code>&amp;geogrid</code> section, edit the <code>geog_data_path</code> and save it.</p> <p></p>"},{"location":"geogrid/#edit-the-geogrid-section-in-namelistwps","title":"Edit the &amp;geogrid section in namelist.wps","text":"<p>Before executing <code>geogrid.exe</code>, edit the information in the namelist according to your case. Read the description for each listed variable in the namelist, as well as best practice here. We will use the information provided in the Resources tab in this tutorial. </p> <p>Export the <code>LD_LIBRARY_PATH</code> and run the <code>geogrid.exe</code>. Ensure you had loaded the <code>intel-oneapi-compilers</code> and <code>intel-oneapi-mpi</code> using <code>spack</code> to avoid the issue of missing <code>libiomp5.so</code>. </p> <pre><code>export LD_LIBRARY_PATH=$(spack location -i netcdf-fortran%intel)/lib/\nspack load intel-oneapi-compilers\nspack load intel-oneapi-mpi\n./geogrid.exe\n</code></pre> <p>If <code>geogrid.exe</code> runs successfully, the following output will be printed: <code>Successful completion of geogrid.</code></p> <p></p> <p>The files that are created by the run will be in the folder. They are shown in the image below:</p> <pre><code>ls\n</code></pre> <p></p>"},{"location":"geogrid/#view-and-adjust-namelistwps-to-fully-cover-the-domain-interested-before-running-the-simulations","title":"View and adjust namelist.wps to fully cover the domain interested before running the simulations","text":"<p>First, open a nice DCV session. Go to the WPS directory, which in this case is <code>/shared/scratch/WPS/</code>. Now we are going to load the <code>ncl</code> package using <code>spack</code>.</p> <pre><code>spack load ncl\n</code></pre> <p>Now, type the following command to view the plot. Pay attention to using the new script as it is written for NCL version 6.2 or later.</p> <pre><code>ncl util/plotgrids_new.ncl\n</code></pre> <p>You have created the plot using DCV! </p> <p>Now repeat the previous process of editing the namelist to adjust the simulation so that it covers the interested domain. Be noted that this is for temporary viewing only. Proceed to the next section if you want to save the file locally.</p> <p></p>"},{"location":"geogrid/#create-plots-in-pdf-format","title":"Create plots in pdf format","text":"<p>To export the image to PDF in the cluster, we will edit the <code>ncl</code> script in the <code>util</code> folder.</p> <pre><code>cd /shared/scratch/WPS/util\nvim plotgrids_new.ncl\n</code></pre> <p>Look for the part where it starts with <code>We generate plots, but what kind do we prefer?</code>. Change the type from <code>x11</code> to <code>pdf</code> and save. After <code>x11</code> describes the title, you can name your file. In this case, we will name it <code>DOMAIN</code>.</p> <p></p> <p>Run the <code>ncl</code> script in the WPS folder again using the DCV session. If successful, you will see a file named DOMAIN.pdf.</p> <p></p> <p>Now, we are going to export the file to the local computer. Copy the <code>DOMAIN.pdf</code> to the home directory.</p> <pre><code>cp DOMAIN.pdf /home/ec2-user\n</code></pre> <p>Export the file from the DCV session by clicking the file storage icon. Choose the file interested, and click <code>Actions</code> to download the file.</p> <p></p> <p></p>"},{"location":"intel_compiler/","title":"Intel Compiler Installation","text":"<p>Run the following command to install Intel Compilers on Spack.</p> <pre><code>spack install --no-cache intel-oneapi-compilers@2022.0.2\n</code></pre> <p>Check that the compiler is installed.</p> <pre><code>spack find\n</code></pre> <p>Load the installed compiler onto Spack so that it recognizes it as a compiler.</p> <pre><code>spack load intel-oneapi-compilers\nspack compiler find\nspack unload\n</code></pre> <p>Then, check the <code>spack compilers</code> command. You should see the Intel compiler that you just installed.</p> <pre><code>spack compilers\n</code></pre> <p></p>"},{"location":"intel_compiler/#install-intel-mpi","title":"Install Intel MPI","text":"<p>Install Intel MPI using the compiler you just installed.</p> <pre><code>spack install --no-cache intel-oneapi-mpi+external-libfabric%intel\n</code></pre>"},{"location":"metgrid/","title":"METGRID","text":"<p>Now, you have reached the final step for the run of WPS. Metgrid will horizontally interpolate the meteorological fields from <code>ungrib</code> to simulation grids defined by <code>geogrid</code>. </p>"},{"location":"metgrid/#export-the-library-path","title":"Export the library path","text":"<p>Just like <code>geogrid</code> and <code>ungrib</code>, export the library path.</p> <pre><code>export LD_LIBRARY_PATH=/shared/spack/opt/spack/linux-amzn2-zen2/intel-2021.5.0/netcdf-fortran-4.5.4-a3txw6pecfmvci7zgwpr3p7nzlqt2k2m/lib\n</code></pre>"},{"location":"metgrid/#load-the-compilers","title":"Load the compilers","text":"<pre><code>spack load intel-oneapi-compilers\nspack load intel-oneapi-mpi\n</code></pre>"},{"location":"metgrid/#run-metgrid","title":"Run metgrid","text":"<p>Run the metgrid. There will be four outputs expected with the name <code>met_em*</code> in this tutorial.</p> <pre><code>./metgrid.exe\n</code></pre> <p></p>"},{"location":"ncl/","title":"NCL Installation","text":""},{"location":"ncl/#install-ncl","title":"Install NCL","text":"<p>Using the binary cache, this should take only around 5 minutes. Once installed, check whether ncl is available. </p> <pre><code>spack install ncl^hdf5@1.8.22\n</code></pre> <pre><code>spack load ncl\nncl -h\n</code></pre> <p></p> <p>Now we set up the NCL X11 window size <code>1000 x 1000</code> to view the WRF output later. </p> .hluresfile<pre><code>cat &lt;&lt; EOF &gt; $HOME/.hluresfile\n*windowWorkstationClass*wkWidth  : 1000\n*windowWorkstationClass*wkHeight : 1000\nEOF\n</code></pre>"},{"location":"resources/","title":"Resources","text":"namelist.inputnamelist.wpsCommon Spack CommandLD_LIBRARY PATHDefine variablesplotgrids_new.ncl <pre><code>cat &lt;&lt;EOF &gt; namelist.input\n &amp;time_control\n run_days                            = 2,\n run_hours                           = 0,\n run_minutes                         = 0,\n run_seconds                         = 0, \n start_year                          = 2022,  2022,\n start_month                         = 04,    04,\n start_day                           = 18,    18,\n start_hour                          = 00,    00,\n start_minute                        = 00,    00,\n start_second                        = 00,    00,\n end_year                            = 2022,  2022,\n end_month                           = 04,    04,\n end_day                             = 20,    20,\n end_hour                            = 00,    00,\n end_minute                          = 00,    00, \n end_second                          = 00,    00,\n interval_seconds                    = 21600\n input_from_file                     = .true.,\n history_interval                    = 720,   60,\n frames_per_outfile                  = 1,     1,\n restart                             = .false.,\n restart_interval                    = 10000,\n io_form_history                     = 2,\n io_form_restart                     = 2,\n io_form_input                       = 2,\n io_form_boundary                    = 2,\n /\n\n &amp;domains\n perturb_input                       = .false.\n time_step                           = 10,\n time_step_fract_num                 = 0,\n time_step_fract_den                 = 1,\n max_dom                             = 2,\n s_we                                = 1,\n e_we                                = 125,   82, \n e_sn                                = 100,   82,\n e_vert                              = 45,    25,\n dx                                  = 15000,\n dy                                  = 15000,\n grid_id                             = 1,     2, \n parent_id                           = 0,     1,\n i_parent_start                      = 1,     50,\n j_parent_start                      = 1,     40,\n parent_grid_ratio                   = 1,     3,\n parent_time_step_ratio              = 1,     3,\n feedback                            = 1,\n smooth_option                       = 0\n num_metgrid_levels                  = 34,\n num_metgrid_soil_levels             = 4,\n /\n\n &amp;physics\n physics_suite                       = 'tropical'\n mp_physics                          = 24,    24,\n ra_lw_physics                       = 1,     1,\n ra_sw_physics                       = 4,     1,\n radt                                = 27,    27,\n sf_sfclay_physics                   = 1,     1,\n sf_surface_physics                  = 2,     2,\n bl_pbl_physics                      = 1,     1,\n bldt                                = 0,     0,\n cu_physics                          = 0,     0,\n cudt                                = 0,     0,\n isfflx                              = 1,\n ifsnow                              = 0,\n icloud                              = 1,\n surface_input_source                = 1,\n num_soil_layers                     = 1,\n maxiens                             = 1,\n maxens                              = 1,\n maxens2                             = 1,\n maxens3                             = 1,\n ensdim                              = 1,\n /\n\n &amp;dynamics\n w_damping                           = 0,\n diff_opt                            = 2,\n km_opt                              = 4,\n khdif                               = 0,\n kvdif                               = 0,\n non_hydrostatic                     = .true.,\n moist_adv_opt                       = 1,\n scalar_adv_opt                      = 1,\n use_baseparam_fr_nml                = .true.,\n /\n\n &amp;bdy_control\n spec_bdy_width                      = 5,\n spec_zone                           = 1,\n relax_zone                          = 4,\n specified                           = .true., \n nested                              = .false.,\n /\n\n &amp;namelist_quilt\n nio_tasks_per_group = 0,\n nio_groups = 1,\n /\nEOF\n</code></pre> <pre><code>cat &lt;&lt;EOF &gt; namelist.wps\n&amp;share\n wrf_core = 'ARW',\n max_dom = 2,\n start_date = '2022-04-18_00:00:00','2022-04-18_00:00:00',\n end_date   = '2021-04-20_00:00:00','2022-04-20_00:00:00',\n interval_seconds = 21600\n/\n\n&amp;geogrid\n parent_id         =   0,   1,\n parent_grid_ratio =   1,   3,\n i_parent_start    =   1,  50,\n j_parent_start    =   1,  40,\n e_we              =  125, 82,\n e_sn              =  100, 82,\n geog_data_res = 'default','default',\n dx = 15000,\n dy = 15000,\n map_proj = 'mercator',\n ref_lat   =  5.300,\n ref_lon   =  100.268,\n truelat1  =  5.300,\n truelat2  =  0,\n stand_lon =  100.268,\n geog_data_path = '/shared/scratch/WRF_Resources/WPS_GEOG/'\n/\n\n&amp;ungrib\n out_format = 'WPS',\n prefix = 'FILE',\n/\n\n&amp;metgrid\n fg_name = 'FILE'\n/\nEOF\n</code></pre> <p>Here list the command that you will use using Spack in this tutorial.</p> <p><pre><code>spack load intel-oneapi-compilers\nspack load intel-oneapi-mpi\n</code></pre> <pre><code>spack load wrf\n</code></pre> <pre><code>spack load wps\n</code></pre> <pre><code>spack find\n</code></pre></p> <p><pre><code>export LD_LIBRARY_PATH=/shared/spack/opt/spack/linux-amzn2-zen2/intel-2021.5.0/netcdf-fortran-4.5.4-a3txw6pecfmvci7zgwpr3p7nzlqt2k2m/lib\n</code></pre> <pre><code>export LD_LIBRARY_PATH=/shared/spack/opt/spack/linux-amzn2-zen2/intel-2021.5.0/jasper-2.0.31-skcu73p6hnlgov6teechaq6muly2xrez/lib64/:\\$LD_LIBRARY_PATH\n</code></pre></p> <p>In case you want to define your variables again...</p> <pre><code>export JASPERLIB=$(spack location -i jasper@2.0.31%intel)/lib64\nexport JASPERINC=$(spack location -i jasper@2.0.31%intel)/include\nexport WRF_DIR=$(spack location -i wrf%intel)\nexport NETCDFINC=$(spack location -i netcdf-fortran%intel)/include\nexport NETCDFLIB=$(spack location -i netcdf-fortran%intel)/lib\nexport NETCDF=$(spack location -i netcdf-fortran%intel)\nexport NETCDFF=$(spack location -i netcdf-fortran%intel)\n</code></pre> <pre><code>;   Script display location of model domains\n;   Only works for ARW domains\n;   Only works for NCL versions 6.2 or later\n;   Reads namelist file directly\n\nload \"$NCARG_ROOT/lib/ncarg/nclscripts/csm/gsn_code.ncl\"\nload \"$NCARG_ROOT/lib/ncarg/nclscripts/wrf/WRFUserARW.ncl\"\n\nbegin\n;\n\n; Check the version of NCL\n  version = systemfunc(\"ncl -V\")\n  if(version.lt.6.2) then\n    print(\"You need NCL V6.2 or later to run this script. Try running plotgrids.ncl. Stopping now...\")\n    return\n  end if\n\n; We generate plots, but what kind do we prefer?\n  type = \"x11\"\n; type = \"pdf\"\n; type = \"ps\"\n; type = \"ncgm\"\n  wks = gsn_open_wks(type,\"wps_show_dom\")\n\n; read the following namelist file\n  filename = \"namelist.wps\"\n\n; Set the colors to be used\n  colors = (/\"white\",\"black\",\"White\",\"ForestGreen\",\"DeepSkyBlue\",\"Red\",\"Blue\"/)\n  gsn_define_colormap(wks, colors)  \n\n; Set some map information ; line and text information\n  mpres = True\n  mpres@mpFillOn = True\n  mpres@mpFillColors  = (/\"background\",\"DeepSkyBlue\",\"ForestGreen\",\"DeepSkyBlue\", \"transparent\"/)\n  mpres@mpDataBaseVersion           = \"Ncarg4_1\"\n  mpres@mpGeophysicalLineColor      = \"Black\"\n  mpres@mpGridLineColor             = \"Black\"\n  mpres@mpLimbLineColor             = \"Black\"\n  mpres@mpNationalLineColor         = \"Black\"\n  mpres@mpPerimLineColor            = \"Black\"\n  mpres@mpUSStateLineColor          = \"Black\"\n;  mpres@mpOutlineBoundarySets       = \"AllBoundaries\"\n  ;mpres@mpGridSpacingF              = 45\n  mpres@tiMainString                = \" WPS Domain Configuration  \"\n\n  lnres = True \n  lnres@gsLineThicknessF = 2.5\n  lnres@domLineColors    = (/ \"white\", \"Red\" , \"Red\" , \"Blue\" /)\n\n  txres = True\n  txres@txFont = \"helvetica-bold\"\n  ;txres@txJust = \"BottomLeft\"\n  txres@txJust = \"TopLeft\"\n  txres@txPerimOn = False\n  txres@txFontHeightF = 0.015\n\n;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n; Do not change anything between the \";;;;;\" lines\n\n  maxdom = 21\n  nvar = 19\n  parent_idn = new (maxdom,integer)\n  parent_grid_ration = new (maxdom,integer)\n  i_parent_startn = new (maxdom,integer)\n  j_parent_startn = new (maxdom,integer)\n  e_wen = new (maxdom,integer)\n  e_snn = new (maxdom,integer)\n  plotvar = new((/maxdom,nvar/),float)\n  plotvar@_FillValue = -999.0\n\n  plotvar = wrf_wps_read_nml(filename)\n\n  mpres@max_dom = floattointeger(plotvar(0,0))\n  mpres@dx = plotvar(0,1)\n  mpres@dy = plotvar(0,2)\n  if (.not.ismissing(plotvar(0,3))) then\n    mpres@ref_lat = plotvar(0,3)\n  else\n    mpres@ref_lat = 0.0\n  end if\n  if (.not.ismissing(plotvar(0,4))) then\n    mpres@ref_lon = plotvar(0,4)\n  else\n    mpres@ref_lon = 0.0\n  end if\n  if (.not.ismissing(plotvar(0,5))) then\n    mpres@ref_x = plotvar(0,5)\n  end if\n  if (.not.ismissing(plotvar(0,6))) then\n    mpres@ref_y = plotvar(0,6)\n  end if\n  mpres@truelat1 = plotvar(0,7)\n  mpres@truelat2 = plotvar(0,8)\n  mpres@stand_lon = plotvar(0,9)\n  mproj_int = plotvar(0,10)\n  mpres@pole_lat = plotvar(0,11)\n  mpres@pole_lon = plotvar(0,12)\n\n  do i = 0,maxdom-1\n    parent_idn(i) = floattointeger(plotvar(i,13))\n    parent_grid_ration(i) = floattointeger(plotvar(i,14))\n    i_parent_startn(i) = floattointeger(plotvar(i,15))\n    j_parent_startn(i) = floattointeger(plotvar(i,16))\n    e_wen(i) = floattointeger(plotvar(i,17))\n    e_snn(i) = floattointeger(plotvar(i,18))\n  end do\n\n  if(mpres@max_dom .gt. 1) then\n    do i = 1,mpres@max_dom-1\n\n      ;Making sure edge is nested grid is at least 5 grid points from mother domain.\n      if(i_parent_startn(i) .lt. 5) then\n        print(\"Warning: Western edge of grid must be at least 5 grid points from mother domain!\")\n      end if\n      if(j_parent_startn(i) .lt. 5) then\n        print(\"Warning: Southern edge of grid must be at least 5 grid points from mother domain!\")\n      end if\n      pointwe = (e_wen(i)-1.)/parent_grid_ration(i)\n      pointsn = (e_snn(i)-1.)/parent_grid_ration(i)\n      gridwe = e_wen(parent_idn(i)-1)-(pointwe+i_parent_startn(i))\n      gridsn = e_snn(parent_idn(i)-1)-(pointsn+j_parent_startn(i))\n      if(gridwe .lt. 5) then\n        print(\"Warning: Eastern edge of grid must be at least 5 grid points from mother domain!\")\n      end if\n      if(gridsn .lt. 5) then\n        print(\"Warning: Northern edge of grid must be at least 5 grid points from mother domain!\")\n      end if\n\n      ;Making sure nested grid is fully contained in mother domain.\n      gridsizewe = (((e_wen(parent_idn(i)-1)-4)-i_parent_startn(i))*parent_grid_ration(i))-(parent_grid_ration(i)-1)\n      gridsizesn = (((e_snn(parent_idn(i)-1)-4)-j_parent_startn(i))*parent_grid_ration(i))-(parent_grid_ration(i)-1)\n      if(gridwe .lt. 5) then\n        print(\"Warning: Inner nest (domain = \" + (i+1) + \") is not fully contained in mother nest (domain = \" + parent_idn(i) + \")!\")\n        print(\"For the current setup of mother domain = \" + parent_idn(i) + \", you can only have a nest of size \" + gridsizewe + \"X\" + gridsizesn + \". Stopping Program!\")\n        exit\n      end if\n      if(gridsn .lt. 5) then\n        print(\"Warning: Inner nest (domain = \" + (i+1) + \") is not fully contained in mother nest (domain = \" + parent_idn(i) + \")!\")\n        print(\"For the current setup of mother domain = \" + parent_idn(i) + \", you can only have a nest of size \" + gridsizewe + \"X\" + gridsizesn + \". Stopping Program!\")\n        exit\n      end if\n\n      ;Making sure the nest ends at a mother grid domain point.\n      pointwetrunc = decimalPlaces(pointwe,0,False)\n      pointsntrunc = decimalPlaces(pointsn,0,False)\n      if((pointwe-pointwetrunc) .ne. 0.) then\n        nest_we_up = (ceil(pointwe)*parent_grid_ration(i))+1\n        nest_we_dn = (floor(pointwe)*parent_grid_ration(i))+1\n        print(\"Nest does not end on mother grid domain point. Try \" + nest_we_dn + \" or \" + nest_we_up + \".\")\n      end if\n      if((pointsn-pointsntrunc) .ne. 0.) then\n        nest_sn_up = (ceil(pointsn)*parent_grid_ration(i))+1\n        nest_sn_dn = (floor(pointsn)*parent_grid_ration(i))+1\n        print(\"Nest does not end on mother grid domain point. Try \" + nest_sn_dn + \" or \" + nest_sn_up + \".\")\n      end if\n\n    end do\n  end if\n\n  mpres@parent_id = parent_idn(0:mpres@max_dom-1)\n  mpres@parent_grid_ratio = parent_grid_ration(0:mpres@max_dom-1)\n  mpres@i_parent_start = i_parent_startn(0:mpres@max_dom-1)\n  mpres@j_parent_start = j_parent_startn(0:mpres@max_dom-1)\n  mpres@e_we = e_wen(0:mpres@max_dom-1)\n  mpres@e_sn = e_snn(0:mpres@max_dom-1)\n\n  if(mproj_int .eq. 1) then\n    mpres@map_proj = \"lambert\"\n    mpres@pole_lat = 0.0\n    mpres@pole_lon = 0.0\n  else if(mproj_int .eq. 2) then\n    mpres@map_proj = \"mercator\"\n    mpres@pole_lat = 0.0\n    mpres@pole_lon = 0.0\n  else if(mproj_int .eq. 3) then\n    mpres@map_proj = \"polar\"\n    mpres@pole_lat = 0.0\n    mpres@pole_lon = 0.0\n  else if(mproj_int .eq. 4) then\n    mpres@map_proj = \"lat-lon\"\n  end if\n  end if\n  end if\n  end if\n\n; Deal with global wrf domains that don't have dx or dy\n\n  if (mpres@dx.lt.1e-10 .and. mpres@dx.lt.1e-10) then\n    mpres@dx = 360./(mpres@e_we(0) - 1)\n    mpres@dy = 180./(mpres@e_sn(0) - 1)\n    mpres@ref_lat = 0.0\n    mpres@ref_lon = 180.0\n  end if\n\n   mp = wrf_wps_dom (wks,mpres,lnres,txres)\n\n;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n\n; Now you can add some information to the plot. \n; Below is an example of adding a white dot over the DC location.\n  ;pmres = True\n  ;pmres@gsMarkerColor = \"White\"\n  ;pmres@gsMarkerIndex = 16\n  ;pmres@gsMarkerSizeF = 0.01\n  ;gsn_polymarker(wks,mp,-77.26,38.56,pmres)\n\n  frame(wks)           ; lets frame the plot - do not delete\n\nend\n</code></pre>"},{"location":"run_wrf/","title":"Running WRF","text":"<p>Congratulations on your successful running of WPS. \ud83e\udd73 </p> <p>Now you are just one step away! </p> <p>Copy the WRF file in a newly created <code>WRF</code> folder in <code>/scratch</code>. You can name the folder into whatever name you want, e.g., <code>WRF1</code>, <code>WRF202210</code>, etc. </p> <pre><code>cd /shared/scratch\nmkdir WRF\ncd WRF\ncp -a $(spack location -i wrf%intel)/. /shared/scratch/WRF\n</code></pre> <p>Go into the <code>WRF/run</code> directory and link the<code>met_em*</code> files. </p> <pre><code>cd run\nln -sf /shared/scratch/WPS/met_em* .\nls -alh\n</code></pre> <p></p> <p>Edit the <code>namelist.input</code> and make necessary changes, or simply overwrite the namelist using the info in the pre-built namelist in the Resources tab. The details, including <code>&amp;time_control</code> and <code>&amp;domains</code> have to be the same as defined in the <code>namelist.wps</code>.You may change the <code>&amp;physics</code> option here too. Refer to UCAR Website for best practices.</p> <pre><code>nano namelist.input\n</code></pre>"},{"location":"run_wrf/#execute-the-realexe-program","title":"Execute the real.exe program","text":"<p>It is always good to reload the compilers every time you run the executables because it might need it, especially after you log off the terminal.</p> <pre><code>spack load intel-oneapi-compilers\nspack load intel-oneapi-mpi\n</code></pre> <p>Load the <code>WRF</code> package before you run. Run WRF to prepare the <code>met_em</code> files by executing <code>real.exe</code>.</p> <pre><code>spack load wrf\nmpirun -np 1 ./real.exe\n</code></pre> <p>Check the <code>rsl.error</code> files to ensure that the run was successful. To do that, you can use the <code>tail</code> command. Usually, the <code>rsl.error.0000</code> will contain the most information. </p> <pre><code>tail rsl.error.0000\n</code></pre> <p>This indicates the successful run of <code>real.exe</code>:</p> <pre><code>real_em: SUCCESS COMPLETE REAL_EM INIT`\n</code></pre> <p></p> <p>Now you are ready to run the <code>wrf.exe</code>. We can use <code>slurm</code> to submit and distribute the tasks to different compute nodes. For <code>slurm</code> to run WRF, ensure that it is in the same directory where your <code>wrf.exe</code> is located. Amend the information according to your needs. </p> slurm-wrf-penang.sh<pre><code>cat &lt;&lt;EOF &gt; slurm-wrf-penang.sh\n#!/bin/bash\n#!/bin/bash\n#SBATCH --job-name=WRF\n#SBATCH --output=penang-%j.out\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=16\n#SBATCH --exclusive\n\nspack load wrf\nset -x\nwrf_exe=$(spack location -i wrf)/run/wrf.exe\nulimit -s unlimited\nulimit -a\n\nexport OMP_NUM_THREADS=6\nexport FI_PROVIDER=efa\nexport I_MPI_FABRICS=ofi\nexport I_MPI_OFI_LIBRARY_INTERNAL=0\nexport I_MPI_OFI_PROVIDER=efa\nexport I_MPI_PIN_DOMAIN=omp\nexport KMP_AFFINITY=compact\nexport I_MPI_DEBUG=4\n\nset +x\nmodule load intelmpi\nset -x\ntime mpirun -np \\$SLURM_NTASKS --ppn \\$SLURM_NTASKS_PER_NODE \\$wrf_exe\nEOF\n</code></pre> <p>Submit the slurm job. You can check the run by <code>squeue</code>.</p> <pre><code>spack load intel-oneapi-compilers\nspack load intel-oneapi-mpi\nspack load wrf\nsbatch slurm-wrf-penang.sh\n</code></pre> <p>Check the <code>rsl.out.0000</code> file to see if the run was successful. </p> <p>You will see a successful message printed at the end of the file.</p> <pre><code>tail rsl.out.0000\n</code></pre> <p></p>"},{"location":"spack/","title":"Spack Installation","text":""},{"location":"spack/#verify-the-cluster","title":"Verify the Cluster","text":"<p>It is always good to verify the cluster information (sinfo, filesystems, etc.) before you start the installation. Check the file systems through this command. </p> <pre><code>df -h\n</code></pre> <p>Check the basic Slurm information. You will notice that all 10 nodes is in idle state because there is no ongoing activity submitted now.</p> <pre><code>sinfo\n</code></pre> <p></p>"},{"location":"spack/#spack-installation_1","title":"Spack Installation","text":"<p>Now, we define the variable <code>SPACK_ROOT</code> and download <code>Spack Version 0.18.0</code> under the <code>/spack</code> folder created.</p> <pre><code>export SPACK_ROOT=/shared/spack\nmkdir -p $SPACK_ROOT\ngit clone -b v0.18.0 -c feature.manyFiles=true https://github.com/spack/spack $SPACK_ROOT\n</code></pre> <p></p> <p>Define the <code>.bashrc</code> file and source it to run the preconfigured spack everytime you open a new terminal.</p> <pre><code>echo \"export SPACK_ROOT=/shared/spack\" &gt;&gt; $HOME/.bashrc\necho \"source \\$SPACK_ROOT/share/spack/setup-env.sh\" &gt;&gt; $HOME/.bashrc\nsource $HOME/.bashrc\n</code></pre> <p>To check your Spack Version, type this or if you want to know more about the Spack you installed, try <code>spack -h</code>. </p> <pre><code>spack -V\n</code></pre> <p>Notice that the version installed is <code>0.18.0</code>.</p> <p>Patchelf Installation</p> <p><code>Patchelf</code> is a small utility to modify the dynamic linker and RPATH of ELF executables.</p> <pre><code>spack install patchelf\n</code></pre> <p>Check the patchelf location and see if it is properly installed. </p> <pre><code>spack find \nspack load patchelf\nwhich patchelf\n</code></pre> <p>Spack build cache</p> <p>Install prerequisites</p> <pre><code>pip3 install botocore==1.23.46 boto3==1.20.46\n</code></pre> <p>Add the mirror to the binary cache</p> Option #1Option #2 <pre><code>spack mirror add binary_mirror  https://binaries.spack.io/releases/v0.18\nspack buildcache keys --install --trust\n</code></pre> <pre><code>spack mirror add aws-hpc-weather s3://aws-hpc-weather/spack/\nspack buildcache keys --install --trust --force \n</code></pre> <p>Adding parallel cluster's softwares to the Spack so that it recognises that these packages are installed.  Copy and paste the following into your CLI. </p> packages.yaml<pre><code>cat &lt;&lt; EOF &gt; $SPACK_ROOT/etc/spack/packages.yaml\npackages:\n    libfabric:\n        variants: fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm\n        externals:\n        - spec: libfabric@1.13.2 fabrics=efa,tcp,udp,sockets,verbs,shm,mrail,rxd,rxm\n          prefix: /opt/amazon/efa\n        buildable: False\nEOF\n</code></pre>"},{"location":"test_wrf/","title":"Testing WRF","text":"<p>To check if the WRF installed can run properly, input data used are 12-km CONUS input. These are used to run the WRF executable (wrf.exe) to simulate atmospheric events that took place during the Pre-Thanksgiving Winter Storm of 2019. The model domain includes the entire Continental United States (CONUS), using 12-km grid spacing, which means that each grid point is 12x12 km. The full domain contains 425 x 300 grid points. After running the WRF model, post-processing will allow visualization of atmospheric variables available in the output (e.g., temperature, wind speed, pressure).</p>"},{"location":"test_wrf/#load-wrf","title":"Load WRF","text":"<p>Load the package WRF in spack.</p> <pre><code>spack load wrf\n</code></pre> <p>Find the executable for <code>WRF</code>. You should expect four executables: <code>real.exe</code>, <code>wrf.exe</code>, <code>ndown.exe</code> and <code>tc.exe</code> for a successful compilation. </p> <pre><code>ls `spack location -i wrf`/run/\n</code></pre> <p>Create scratch folder under <code>/shared</code>.</p> <pre><code>cd /shared\nmkdir scratch\ncd scratch\n</code></pre> <p>Download the file.  </p> <pre><code>curl -O https://www2.mmm.ucar.edu/wrf/OnLineTutorial/wrf_cloud/wrf_simulation_CONUS12km.tar.gz\ntar -xzf wrf_simulation_CONUS12km.tar.gz\n</code></pre> <p>Next we\u2019ll prepare the data for a run by copying in the relevant files from our WRF install.It is safe to ignore the error message <code>ln: failed to create symbolic link \u2018./namelist.input\u2019: File exists</code> since there is existing namelist.input file that contains all the necessary information for the run.</p> <pre><code>cd conus_12km\nWRF_ROOT=$(spack location -i wrf%intel)/test/em_real/\nln -s $WRF_ROOT* .\n</code></pre> <p>Let's create a Slurm job submission script in the conus_12km. This file specifies the job submission script as follows:</p> Resource No. Description <code>--nodes</code> 2 Using 2x Hpc6a.48xl instances <code>--ntasks-per-node</code> 16 Each node has 16 MPI processes <code>OMP_NUM_THREADS</code> 6 Each node has 6 OpenMP threads <p>slurm-wrf-conus12km.sh<pre><code>cat &lt;&lt;EOF &gt; slurm-wrf-conus12km.sh\n#!/bin/bash\n#!/bin/bash\n#SBATCH --job-name=WRF\n#SBATCH --output=conus-%j.out\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=16\n#SBATCH --exclusive\n\nspack load wrf\nset -x\nwrf_exe=\\$(spack location -i wrf)/run/wrf.exe\nulimit -s unlimited\nulimit -a\n\nexport OMP_NUM_THREADS=6\nexport FI_PROVIDER=efa\nexport I_MPI_FABRICS=ofi\nexport I_MPI_OFI_LIBRARY_INTERNAL=0\nexport I_MPI_OFI_PROVIDER=efa\nexport I_MPI_PIN_DOMAIN=omp\nexport KMP_AFFINITY=compact\nexport I_MPI_DEBUG=4\n\nset +x\nmodule load intelmpi\nset -x\ntime mpirun -np \\$SLURM_NTASKS --ppn \\$SLURM_NTASKS_PER_NODE \\$wrf_exe\nEOF\n</code></pre> Submit the run using the Slurm Workload Manager.</p> <pre><code>sbatch slurm-wrf-conus12km.sh\n</code></pre>"},{"location":"test_wrf/#dcv","title":"DCV","text":"<p>Open a NICE DCV session through PCluster Manager.Once you are connected over NICE DCV, open the terminal and execute the following.</p> <pre><code>cd /shared/scratch/conus_12km \nspack load ncl\nncl ncl_scripts/surface.ncl\n</code></pre> <p></p> <p>You can also generate a vertical profile of relative humidity (%) and temperature (K).</p> <pre><code>cd /shared/scratch/conus_12km \nspack load ncl    \nncl ncl_scripts/vert_crossSection.ncl\n</code></pre>"},{"location":"ungrib/","title":"UNGRIB","text":"<p>Create GFS data under <code>/WRF_Resources</code> that you just created. </p> <pre><code>cd /shared/scratch/WRF_Resources\nmkdir GFS_Data\ncd GFS_Data\n</code></pre> <p>Before you start using the Python script, check the website Research Data Archive website to see if the data sets for your interested dates are available. Create a Python script to download the GFS Data. </p> gfs_downloader.py<pre><code>cat &lt;&lt;EOF &gt; gfs_downloader.py\n#!/usr/bin/env python\nimport sys, os\nimport requests\nfrom datetime import datetime, timedelta\ndef download_file(session, filename, cookies, output_file_path):\nprint('Downloading', os.path.basename(filename))\nreq = session.get(filename, cookies=cookies, allow_redirects=True, stream=True)\nwith open(output_file_path, 'wb') as outfile:\nfor chunk in req.iter_content(chunk_size=32768):  # Increased chunk size for potentially faster downloads\noutfile.write(chunk)\nfile_size = os.path.getsize(output_file_path) / (1024 * 1024)\nprint(f\"{os.path.basename(filename)} download completed. File size: {file_size:.2f} MB\")\ndef generate_filelist(start_date, end_date, hours_interval):\ns_date = datetime.strptime(start_date, \"%Y/%m/%d\")\ne_date = datetime.strptime(end_date, \"%Y/%m/%d\")\n# Generate the list of dates between start and end dates\ndates = []\nwhile s_date &lt;= e_date:\ndates.append(s_date)\ns_date += timedelta(days=1)\n# Generate the file paths based on the given hour interval and the dates\nfilepaths = []\nfor idx, date in enumerate(dates):\nmax_hour = 24 if idx != len(dates)-1 else 1\nfor hour in range(0, max_hour, hours_interval):\nfilepath = \"{}/{}/gfs.0p25.{}{:02d}.f000.grib2\".format(\ndate.strftime(\"%Y\"),\ndate.strftime(\"%Y%m%d\"),\ndate.strftime(\"%Y%m%d\"),\nhour\n)\nfilepaths.append(filepath)\nreturn filepaths\ndef check_file_status(filepath, filesize):\nsys.stdout.write('\\r')\nsys.stdout.flush()\nsize = int(os.stat(filepath).st_size)\npercent_complete = (size/filesize)*100\nsys.stdout.write('%.3f %s' % (percent_complete, '% Completed'))\nsys.stdout.flush()\n# Check for 3 arguments now (script name, username, and password)\nif len(sys.argv) &lt; 3 and not 'RDAUSR' in os.environ: \ntry:\nimport getpass\nexcept:\ntry:\ninput = raw_input  # For Python 2\nexcept:\npass\nusr = input('Username: ')\npswd = getpass.getpass('Password: ')  # Use getpass just for the password\nelse:\ntry:\nusr = sys.argv[1]\npswd = sys.argv[2]\nexcept:\nusr = os.environ['RDAUSR']\npswd = os.environ['RDAPSWD']\nurl = 'https://rda.ucar.edu/cgi-bin/login'\nvalues = {'email' : usr, 'passwd' : pswd, 'action' : 'login'}\n# Authenticate\nret = requests.post(url,data=values)\nif ret.status_code != 200:\nprint('Bad Authentication')\nprint(ret.text)\nexit(1)\ndspath = 'https://data.rda.ucar.edu/ds084.1/'\n# Get user input using standard input\nstart_date = input(\"start date (format: YYYY/MM/DD): \")\nend_date = input(\"end date (format: YYYY/MM/DD): \")\nhours = int(input(\"hours interval (e.g., 6): \"))\nfilelist = generate_filelist(start_date, end_date, hours)\nfor item in filelist:\nprint(item)\nprint('\\n')\n# Create a directory based on the start and end dates\nfolder_name = \"{}_{}\".format(start_date.replace(\"/\", \"-\"), end_date.replace(\"/\", \"-\"))\nif not os.path.exists(folder_name):\nos.makedirs(folder_name)\nsession = requests.Session()\nfor file in filelist:\nfilename = dspath + file\nfile_base = os.path.basename(file)\noutput_file_path = os.path.join(folder_name, file_base)\n# Download files sequentially\ndownload_file(session, filename, ret.cookies, output_file_path)    \nEOF\n</code></pre> <p>Execute the Python script to download the data. You must be a registered user on the NCAR website because you will be asked for a password before permission to download the data is granted. After executing the Python code, enter your NCAR website username and password. Then, input the start date, end date, and hourly time interval; the code will then generate a list of files to download.</p> <pre><code>pip3 install requests    \npython3 gfs_downloader.py\n</code></pre> <p>Obtain the path to your <code>GFS_Data</code>, where, in this case, it is <code>/shared/scratch/WPS</code>. Go back to the directory where your WPS is compiled and create the symbolic links to the GFS data that you just downloaded. You should expect four GRIBFILE with the same prefix but different labels behind, <code>AAA</code>, <code>AAB</code>, <code>AAC</code> and <code>AAD</code>.</p> <pre><code>cd /shared/scratch/WPS\n./link_grib.csh /shared/scratch/WRF_Resources/GFS_Data/folder_name/gfs.*\n</code></pre> <p>Create the symbolic link to the Vtable.</p> <pre><code>ln -sf ungrib/Variable_Tables/Vtable.GFS Vtable\nls -alh\n</code></pre> <p></p> <p>Now, it is time to edit the <code>namelist.wps</code>. Below are a few things that should be amended according to the GFS data: </p> <ul> <li>start date </li> <li>end date </li> <li>the interval seconds that describe the interval of your GFS data</li> </ul> <pre><code>nano namelist.wps\n</code></pre> <p>Increase your stack limit as the superuser.</p> <pre><code>sudo -s\nulimit -s unlimited\n</code></pre> <p>Export the library path.</p> <pre><code>export LD_LIBRARY_PATH=/shared/spack/opt/spack/linux-amzn2-zen2/intel-2021.5.0/jasper-2.0.31-skcu73p6hnlgov6teechaq6muly2xrez/lib64/:\\$LD_LIBRARY_PATH\n</code></pre> <p>Run the <code>ungrib</code>.</p> <pre><code>./ungrib.exe\n</code></pre> <p>You will see the successful output printed out after <code>ungrib.exe</code> and four outputs with the prefix <code>FILE</code> are created.</p> <p> </p> <p>Remember to exit the superuser by typing exit.</p> <pre><code>exit\n</code></pre>"},{"location":"wps/","title":"WPS Installation","text":"<p>Now, we will be creating a Slurm job submission script to install WPS using Spack.</p> wps-install.sbatch<pre><code>cat &lt;&lt;EOF &gt; wps-install.sbatch\n#!/bin/bash\n#SBATCH -N 1\n#SBATCH --exclusive\n\nspack install -j 1 wps%intel ^intel-oneapi-mpi+external-libfabric\nEOF\n</code></pre> <p>Submit the job by copy the following to your CLI.</p> <pre><code>sbatch wps-install.sbatch\n</code></pre>"},{"location":"wps/#checking-the-clusters","title":"Checking the Clusters","text":"<p>You may check the progress of the installation. <code>CF</code> indicates that the nodes is currently under configuration while <code>R</code> indicates running nodes.</p> <pre><code>squeue\n</code></pre> <p></p> <p>Use <code>cat</code> or <code>tail</code> to see the output of the installation. You shall see the successful installation message at the end of the output upon successful installation.</p> cattail <pre><code>cat slurm-&lt;jobID&gt;.out\n</code></pre> <pre><code>tail slurm-&lt;jobID&gt;.out\n</code></pre> <p>Use Spack find to check if the version of WPS is correctly installed under the intel compilers.</p> <p></p> <pre><code>spack find\n</code></pre> <p></p>"},{"location":"wps_configuration/","title":"WPS Configuration","text":""},{"location":"wps_configuration/#create-a-soft-link-for-the-wps","title":"Create a soft link for the WPS","text":"<p>Load the compilers before you do anything.</p> <pre><code>spack load intel-oneapi-compilers\nspack load intel-oneapi-mpi\n</code></pre> <p>Warning</p> <p>Copy the WPS folder instead of using soft links.</p> <pre><code>cd /shared/scratch\nmkdir WPS\ncp -a $(spack location -i wps%intel)/. WPS/\ncd WPS\nls\n</code></pre>"},{"location":"wps_configuration/#jasper-installation","title":"Jasper Installation","text":"<p>Jasper is needed for WPS, so you have to install it.</p> <pre><code>spack install jasper@2.0.31%intel\n</code></pre> <p>Check if the <code>jasper version 2.0.31</code> is installed.</p> <pre><code>spack find\n</code></pre> <p></p>"},{"location":"wps_configuration/#export","title":"Export","text":"<p>Export the paths needed for WPS to work.</p> <pre><code>export JASPERLIB=$(spack location -i jasper@2.0.31%intel)/lib64\nexport JASPERINC=$(spack location -i jasper@2.0.31%intel)/include\nexport WRF_DIR=$(spack location -i wrf%intel)\nexport NETCDFINC=$(spack location -i netcdf-fortran%intel)/include\nexport NETCDFLIB=$(spack location -i netcdf-fortran%intel)/lib\nexport NETCDF=$(spack location -i netcdf-fortran%intel)\nexport NETCDFF=$(spack location -i netcdf-fortran%intel)\nexport LD_LIBRARY_PATH=/shared/spack/opt/spack/linux-amzn2-zen2/intel-2021.5.0/jasper-2.0.31-skcu73p6hnlgov6teechaq6muly2xrez/lib64=/shared/spack/opt/spack/linux-amzn2-zen2/intel-2021.5.0/netcdf-fortran-4.5.4-izf5fn4mw4y4mcgdjed7jp7bypsfi3s2/lib:\\$LD_LIBRARY_PATH\n</code></pre> <p>Check if the variables are imported correctly.</p> <pre><code>echo $NETCDFINC\necho $NETCDFLIB\n</code></pre> <p>Copy the contents of <code>lib</code> and include <code>netcdf-c</code> to <code>netcdf-fortran</code> of the Intel compilers.</p> <pre><code>cp -a $(spack location -i netcdf-c%intel)/include/. $(spack location -i netcdf-fortran%intel)/include/\ncp -a $(spack location -i netcdf-c%intel)/lib/. $(spack location -i netcdf-fortran%intel)/lib/\n</code></pre>"},{"location":"wps_configuration/#edit-configurewps-file","title":"Edit configure.wps File","text":"<p>Configure the WPS installation.</p> <pre><code>./configure\n</code></pre> <p>Choose '17', for Intel serial compilers.</p> <p>If you did not set the path for <code>NETCDFINC</code>, type <code>Y</code>, then paste the <code>NETCDFINC</code> paths as given above.</p> <pre><code>nano configure.wps\n</code></pre> <p>Look for <code>WRF_LIB</code> and add <code>-qopenmp</code> after <code>-lnetcdff -lnetcdf</code> then save.</p> <p></p> <p>Before compiling, notice the original copied files of WPS. You can view them by typing:</p> <pre><code>ls\n</code></pre> <p></p>"},{"location":"wps_configuration/#compile","title":"Compile","text":"<p>The compilation of the executables can begin. The files that will be created are <code>geogrid.exe</code>, <code>metgrid.exe</code>, and <code>ungrib.exe</code>.</p> <pre><code>./compile &gt; log.compile\n</code></pre> <p></p> <p>After compiling, you should see new files created in the folder. You can view them by typing:</p> <pre><code>ls\n</code></pre> <p></p>"},{"location":"wps_configuration/#clean","title":"Clean","text":"<p>If you want to start over the creation of the WPS executables, you can use the command below. Note that it will not delete any files, e.g., <code>met_em</code>, <code>.nc</code>, etc., other than those created in the compilation process.</p> <p>Note</p> <p>to reset WPS settings!</p> <pre><code>./clean -a\n</code></pre>"}]}